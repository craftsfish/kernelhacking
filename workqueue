workqueue_struct和具体的worker_pool的关联是通过pool_workqueue来实现的，是一种多对多的关系，pool_workqueue就是连接表
workqueue_struct可以创建任意多个，根据具体的属性与worker_pool关联
所有的unbound_pool都存放在unbound_pool_hash，pool->attrs(nice+cpumask)作为hash key
每个cpu有NR_STD_WORKER_POOLS(2)个worker_pool分别对应不同的优先级
ordered_unbound_pool的唯一性约束是通过构造pwq的时候，所有的node都指向default pwq来确保的

lock {
	wq->mutex
	pwq->pool->lock
}

workqueue建立以后可以通过sysfs接口动态的调整nice

pool的并发控制 {
	pool内控制并发，目标是有work要做的时候，一个worker在跑就行了
	worker中的work如果suspend的时候，会勾住sched_core的接口，判断是否有其他work并且没有worker在跑，如果满足该条件，启动新的worker
	并发的worker执行完成后，会判断当前的running worker是否超过1个，如果是的话，会进入idle状态
}

flush机制 {
	增加一个barrier work链接到target work后面
	worker遇到linked work的时候，会依次把linked work以及之后的一个work移到scheduled队列里面
	如果target work本身已经在worker上面，那么直接把barrier work插入scheduled队列
}

cpu hotplug {
}

unbound_pool_hash管理所有的unbound pool，通过wq_pool_mutex同步，如果有重复的可以利用，直接pool->refcnt++

wq->numa_pwq_tbl为什么要wq_pool_mutex和wq->mutex同时保护? {
	原始的方案只需要wq->mutex
	下面的patches引入了wq_pool_mutex {
		commit 899a94fe15a8e928277ff0d0402c086fa67fe16f
		commit da7f91b2e2c6176f95ca7b538d74dc70c5d11ded
		commit d4d3e2579756e3a5f4fbf8eac211f0696e253bcd
		commit a0111cf6710bd1b4145ef313d3f4772602af051b
		commit f7142ed483f49f9108bea1be0c1afcd5d9098e05
		commit 5b95e1af8d17d85a17728f6de7dbff538e6e3c49
	}
	虽然后续的提交没有指出，但是原始的方案应该是有问题的，从代码分析来看，wq的unbound相关属性是由pool和wq同时决定的，所以在修改的时候必须同时获取两把锁。
	cpu online/offline会影响pool，并进一步影响wq的unbound属性

	wq_cpumask_store的效果被cpu online完全覆盖了，具体CPU如何影响unbound属性需要继续调查
	cpu online                           |          wq_cpumask_store
	mutex_lock(&wq_pool_mutex);          |
	update pool                          |
	                                     |          mutex_lock(&wq->mutex);
	                                     |          numa_pwq_tbl_install
	                                     |          mutex_unlock(&wq->mutex);
	mutex_lock(&wq->mutex);              |
	numa_pwq_tbl_install                 |
	mutex_unlock(&wq->mutex);            |
	mutex_unlock(&wq_pool_mutex)         |
}


wq_unbound_cpumask

unbound workqueue包含default_pwq和每node pwq

unbound workqueue的max_active是如何保证的？

flush_color < work_color ?

pool {
	cpu, per_cpu_pool -> default cpu, unbound pool -> -1
}

work被queue之后会优先放到对应的pool里面，如果pool里面包含的work数量大于等于pwd的max_active数，work会被缓存在pwd的delayed_works中

unbound_pwq install的时候值为1
unbound_pwq uninstall的时候值为-1
unbound_workqueue更新属性时会uninstall old_pwq
insert_work时get_pwq
work完成时调用pwq_dec_nr_in_flight -> put_pwq

unbound pwq对pool的引用计数在pwq_unbound_release_workfn -> put_unbound_pool释放
